# -*- coding: utf-8 -*-
"""OpenAI Gym Environment Taxi Game Agent training using Reinforcement Learning.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/173zaXjj7LTH-zPpoQqGlFX84M3K39iBi
"""

import gym
import numpy as np
import tensorflow as tf
from keras.utils import to_categorical
import matplotlib.pyplot as plt
import random

# Function to render the environment
def render(env):
    plt.imshow(env.render(mode='rgb_array')[0])
    plt.show()

# Create the Taxi environment
env = gym.make("Taxi-v3", render_mode='rgb_array')
env.reset()
render(env)

# Get the action and state size of the environment
action_size = env.action_space.n
print("Action size: ", action_size)

state_size = env.observation_space.n
print("State size: ", state_size)

# Q-Network model definition
model = tf.keras.models.Sequential([
    tf.keras.layers.InputLayer(input_shape=(state_size,)),
    tf.keras.layers.Dense(500, activation='relu'),
    tf.keras.layers.Dense(action_size, activation='linear')
])
model.compile(optimizer=tf.keras.optimizers.Adam(), loss='mse')
model.summary()

# Evaluation policy for Q Network
def eval_dqn(env, model, num_of_episodes, max_steps, show=False):
    rewards = np.empty(num_of_episodes)
    for e in range(num_of_episodes):
        state = env.reset()
        val = 0
        for t in range(max_steps):
            action = np.argmax(model.predict(tf.one_hot([state, state], state_size), verbose=0)[0])
            state, reward, done, info = env.step(action)[:4]
            val += reward
            if show:
                render(env)
            if done:
                break
        rewards[e] = val
    return np.mean(rewards), np.min(rewards), np.max(rewards), np.std(rewards)

# Evaluation policy for Q Learning
def eval_policy(env, q_table, gamma, iterations, episodes, show=False):
    val_r = np.empty(episodes)
    for e in range(episodes):
        state = env.reset()
        val = 0
        for t in range(iterations):
            action = np.argmax(q_table[state])
            state, reward, done, info = env.step(action)[:4]
            val += reward
            if show:
                render(env)
            if done:
                break
        val_r[e] = val
    return np.mean(val_r), np.min(val_r), np.max(val_r), np.std(val_r)

# Q-Learning setup
q_table = np.zeros((state_size, action_size))
episodes = 3000
interactions = 100
epsilon = 1
alpha = 0.5
gamma = 0.9
q_learning_hist = []

# Q-Learning loop
for episode in range(episodes):
    state = env.reset()
    done = False
    epsilon = 0.99 ** (episode / 5)

    for interact in range(interactions):
        # Exploration-exploitation trade-off
        if np.random.uniform(0, 1) > epsilon:
            action = np.argmax(q_table[state, :])
        else:
            action = np.random.randint(0, action_size)

        new_state, reward, done, info = env.step(action)[:4]

        # Update the Q-value
        q_table[state, action] = q_table[state, action] + alpha * (
                reward + gamma * np.max(q_table[new_state, :]) - q_table[state, action])

        state = new_state

        if done:
            break

    # Q-Learning Evaluation
    if episode % 25 == 0 or episode == 1:
        val_mean, val_min, val_max, val_std = eval_policy(env, q_table, gamma, 20, 500)
        q_learning_hist.append([episode, val_mean, val_min, val_max, val_std])
        print(f'Q Learning mean reward for episode {episode} is {val_mean} \n')

# Plot Q-Learning results
env.reset()
q_learning_hist = np.array(q_learning_hist)
print(q_learning_hist.shape)
fig, ax = plt.subplots()
ax.plot(q_learning_hist[:, 0], q_learning_hist[:, 1])
ax.fill_between(q_learning_hist[:, 0], (q_learning_hist[:, 1] - q_learning_hist[:, 4]),
                (q_learning_hist[:, 1] + q_learning_hist[:, 4]), color='b', alpha=.1)
plt.show()

# One-hot encoding for Q Network input
x = np.arange(500)
x_one_hot = to_categorical(x, 500)

# Q Network Learning and Evaluation
q_network_hist = []

for ep in range(700):
    index = random.sample(range(500), 400)
    model.fit(x_one_hot[index], q_table[index, :], epochs=15, verbose=1)

    if ep % 25 == 0 or ep == 1:
        dnq_mean, dnq_min, dnq_max, dnq_std = eval_dqn(env, model, 10, 50)
        q_network_hist.append([ep, dnq_mean, dnq_std])
        print(f'Q Network mean reward for episode {ep} is {dnq_mean} \n')

# Plot Q Network results
q_network_hist = np.array(q_network_hist)
print(q_network_hist.shape)
fig, ax = plt.subplots()
ax.plot(q_network_hist[:, 0], q_network_hist[:, 1])
ax.fill_between(q_network_hist[:, 0], (q_network_hist[:, 1] - q_network_hist[:, 2]),
                (q_network_hist[:, 1] + q_network_hist[:, 2]), color='b', alpha=.1)
plt.show()